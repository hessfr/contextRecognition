\chapter{Motivation}\label{cha1}
Recent technological advances of mobile devices and exponentially increasing user numbers have created a new research area to quantitatively (??) predict the context of users. A wide range of sensors included in modern smart phone can be utilized for this purpose including accelerometers, gyroscopes, GPS, Wifi, cameras, proximity sensors, microphones etc.\\
This additional information gives rise to a new kind of applications in various domains like public transport \cite{Thiagarajan2014}, healthcare \cite{Bricon-Souf2007} or social science \cite{Eagle2005} can provide the basis for more intuitive human-computer interaction \cite{Schmidt1999}.\\
Most recognition system include many different sensor modalities and try to fuse them in a reasonable way. Nevertheless researchers also try to improve recognition results of individual sensor modalities independent from each other.\\
In this work we will focus on how to use the contextual cues contained in ambient sound to predict the context of a user. Much of this information is not existing in other modalities (e.g. speech) and can therefore provide deeper insights of user contexts.\\
A common challenge of many machine learning application is how to obtain data to train the algorithm without putting too much burden on the end user to collect / create this data.\\
In this thesis we used sound files from the crowd-sourced sound repository Freesound\footnote{www.freesound.org} to initially train the classifier. However this crowd-sourced data can be different from the sound encountered by individual users. To fit this general model to specific users, stream-based active learning is used to query end users about their actual context from time to time and incorporate this information into the recognition system. 







